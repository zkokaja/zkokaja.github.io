---
layout: post
title: "Dialogue: K-means"
date: 2018-05-03 18:10:00
categories: [cs, math, ml]
tags: [math,cs,ml]
---

A playful introduction and application to k-means in the spirit of [Lewis
Carroll][1], [Plato][2], and [Hofstadter][3]: using a conversation to explain a
concept. I've chosen to use a Fox and a Hedgehog as the actors in this
conversation, motivated by Archilochus' [quote][5]: 
> The fox knows many things; the hedgehog one big thing.

The Fox and the Hedgehog are looking at an
image of the galaxies found by the [Hubble Telescope][4].

<img src='/images/2018-kmeans-heic1.png' class='center-image' width="500px" alt="Hubble"/>

<script
src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
type="text/javascript"></script>

_Fox_: That is one of my favorite pictures. Did you know that they pointed the
_space_ telescope to the darkest patch of the sky, unsure what they would get,
and they discovered thousands of galaxies!

_Hedgehog_: It really is remarkable, truly commendable of science. I wonder how
many galaxies are in it, Fox. What do you think? 

_Fox_: I can think of many ways to calculate it. For one, we can give it to
Turtle with a magnifying glass - but that would take a while. We can get a rough
estimate just by counting the bright pixels and dividing by the average number
of pixels in a cluster.

_Hedgehog_: That's a reasonable start. But you just reminded me of clustering
algorithms, maybe we can apply that here?

_Fox_: Oh, you mean like
[k-means](https://en.wikipedia.org/wiki/K-means_clustering)? I have heard of it,
but don't know much about it. Maybe you can explain it to me.

_Hedgehog_: Sure! Hmm, let's start with a simple example first. Let me throw some
pebbles on the ground... here we go, take a look: 

<img src='/images/2018-kmeans-1.png' class='center-image' width="400px"
alt="Three clusters of points on a 2D plot."/>

_Fox_: Yes, I can immediately see three distinct clusters.

_Hedgehog_: Good, but our algorithm doesn't yet. It's pretty simple though, just an
iterative two-step process. But before that, we have these pine cones here,
let's place them randomly on our graph:

<img src='/images/2018-kmeans-2.png' class='center-image' width="400px"
alt="Three clusters of points on a 2D plot."/>

They are our means. Note that we used k=3 pine cones here because we know
beforehand how our pebbles are structured.

_Fox_: The means of each cluster? But they're nowhere close to their centers!

_Hedgehog_: That's right, but with each iteration they will get closer. The first
step is called Assignment. For each pine cone - um, mean - we assign it the
pebbles which are closest to it.

_Fox_: I see. In other words, for each pebble we check which pine cone is
closest to it, using a ruler or some sort of euclidean distance metric, and say
the pebble is part of its cluster.

_Hedgehog_: Exactly! Mathemtically this is

$$c_i = \min_k{||x_i - \mu_k||^2}$$

But you can see they're not very accurate yet:

<img src='/images/2018-kmeans-3.png' class='center-image' width="400px"
alt="Three clusters of points on a 2D plot."/>

So the next step is called Update, and it's where we move the pine cones so
they're at the average of all the points assigned to it.

_Fox_: That is simple, we're just taking the mean of the assigned points. So, we
can write it like this:

$$\mu_k = \frac{1}{m} \sum_{i:c_i=k}{x_i}$$

_Hedgehog_: You're quick, Fox! You should see if we repeat this process a few times,
the pine cones will be in the middle of each pebble clusters. That is, the means
converge to the cluster centroids.

<video width="432" height="288" class="center-image" loop controls autoplay>
  <source src="/images/2018-kmeans-animation.mp4" type="video/mp4">
</video>

_Fox_: Very cool. This sounds like a useful algorithm. But how does it apply to
our picture problem?

_Hedgehog_: It is very useful and appears in many fields: machine learning,
signal processing, and image processing (compression, segmentation). Now, for
our picture, we can think of each bright pixel as a pebble, and we want to find
the means of as many as we can.

_Fox_: I see, but how many pine cones do we choose, and how do we place them? It
seems like there are a lot of different sized galaxies.

_Hedgehog_: Excellent questions. For this example we can estimate a number and then
confirm it visually; in practice there are some more practical ways - but
it's not always easy! We have to be careful not to place too many pine cones
together, or some won't be assigned any pebbles! A potential method to mitigate
that would be to choose random pebbles and put a pine cone on it. There are
other ways to handle these questions, and they definitely affect our
performance!

But for now, we will let The Hands transform the image into an appropriate
input for the algorithm, and let it process for a while (this is a large image
after all). I think a good starting choice for k is 10,000 because that's what
the owner of the picture claims.

And finally we get the clusters, here they are:

<img src='/images/2018-kmeans-hubble-mu.png' class='center-image' width="700px"
alt="Three clusters of points on a 2D plot."/>

_Fox_: Wow! Each of those points is a mean that found the center of a cluster
of bright pixels. Ten thousand in total. Can we see how this compares with the
original data?

_Hedgehog_: Of course. I'll show you the pixel data colored by cluster assignment.
Unfortunately there are too many to identify uniquely (or that our eyes can
distinguish) thus some are reused.

<img src='/images/2018-kmeans-hubble-clusters.png' class='center-image' width="700px"
alt="Three clusters of points on a 2D plot."/>

This is pretty picture. However, it's likely our 10,000 means _overfit_ the
data, meaning that most of our points had a corresponding mean to explain them.
Nevertheless, it could be due to the low resolution of our image that there just
weren't as many bright pixels. Or the way which we processed the image
initially. Our original image had roughly 15,000 bright pixels, so maybe we should
choose a smaller k.

_Fox_: That makes sense. How about a third of it, 5,000 means?

_Hedgehog_: Let's try that:

<img src='/images/2018-kmeans-hubble-5k.png' class='center-image' width="700px"
alt="Three clusters of points on a 2D plot."/>

_Fox_: This looks quite good, similar to our other results but with less
overfitting. Although I'm still willing to believe there are more than 5000
galaxies in the original picture.

_Hedgehog_: Indeed, if we had a larger picture, we would have more data points,
thus the galaxies even farther away can be represented. And with a more general
algorithm like EM we could represent clusters with a Gaussian, so we would
estimate variance in addition to the mean. Maybe for next time, but for now I
think you have a good idea of k-means, Fox!

---

## Appendix

I realize this algorithm may not be the ideal choice for the problem I'm
presenting here, but I think it's a interesting problem and gives me the
opportunity to discuss it in an fun way.

I wanted to share some of the Python code I used for the post. This is a
non-vectorized implementation because it's easier to understand as an
introduction. Here is the assignment step:

```python
def assign(x, mu, k):
    n = x.shape[0]
    c = np.zeros(n, np.int)
    
    for i in range(n):
        min_dist = np.inf

        for j in range(k):
            dist = np.sum((x[i] - mu[j])**2)
            if dist < min_dist:
                min_dist = dist
                c[i] = j

    return c
```

And the Update step:

```python
def update(x, r, k):
    mu = np.zeros((k,2))
    for i in range(k):
        y = np.where(r == i)[0]
        mu[i] = np.sum(x[y], axis=0) / y.size
        
    return mu
```

The animation was a bit tricky because it waas the first time I created one with
matplotlib, so here's the code for it:

```python
k = 3
fig, ax = plt.subplots()
scat = ax.scatter([], [])

def update_plot(i):
    global mu
    ax.clear()
    ax.axis('off')
    # May need to set xlim and ylim
    
    if i == 0: mu = np.array([[-2,2], [2,0], [8,0]])   

    r = assign(x, mu, k)
    ax.scatter(x[:,0], x[:,1], c=r, marker='.', s=20)
    ax.scatter(mu[:,0], mu[:,1], c='k', marker='x', s=80)
    mu = update(x, r, k)
    
    return scat,

anim = animation.FuncAnimation(fig, update_plot, frames=3, \
    interval=1500, blit=True)
HTML(anim.to_html5_video())
```

For the image, I relied on scikit-learn for their k-means implementation. And I
had to threshold the image to get a binary representation; which I extracted the
coordinates of all the positive examples:

```python
from skimage import io
from sklearn.cluster import k_means
import matplotlib.pyplot as plt

bw = io.imread('heic.png', as_grey=True)
th = np.zeros(bw.shape, np.bool)
th[bw > .15] = True

x,y = np.where(th == True)
X = np.zeros((x.shape[0],2), x.dtype)
X[:,0] = x
X[:,1] = y

mus, _, _ = k_means(X, 5000)
plt.scatter(mus[:,0], mus[:,1], s=1, c='r', alpha=1); 
plt.show()
```

[1]: https://en.wikipedia.org/wiki/What_the_Tortoise_Said_to_Achilles
[2]: https://en.wikipedia.org/wiki/Plato#Dialogues
[3]: https://en.wikipedia.org/wiki/G%C3%B6del,_Escher,_Bach
[4]: https://www.spacetelescope.org/images/heic0406a/
[5]: https://en.wikipedia.org/wiki/Archilochus#Quotes
